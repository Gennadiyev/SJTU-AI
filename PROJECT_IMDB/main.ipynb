{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da58061d",
   "metadata": {},
   "source": [
    "# First Attempt implementing FC neural network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274cd885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # God please write code for me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb964720",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "The aim of the model is to estimate the [IMDB](https://www.imdb.com) score of the a movie, based on:\n",
    "\n",
    "- Name\n",
    "- Release Date\n",
    "- Production Budget\n",
    "- Genre\n",
    "- Worldwide Gross\n",
    "- Run Time\n",
    "\n",
    "We then try to identify this relationship.\n",
    "\n",
    "$$\\left.\n",
    "\\begin{array}{l}\n",
    "\\text{Film Name} \\\\\n",
    "\\text{Release Date} \\\\\n",
    "\\text{Production Budget} \\\\\n",
    "\\text{Genre} \\\\\n",
    "\\text{Worldwide Gross} \\\\\n",
    "\\text{Run Time}\n",
    "\\end{array}\n",
    "\\right\\}\\Rightarrow\\text{IMDB Rating}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac65ed7",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "The data source comes from [here](https://vega.github.io/editor/data/movies.json), licensed under MIT License.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "For preprocessing, I used a combination of Lua and Python.\n",
    "\n",
    "### Filter\n",
    "\n",
    "The original JSON file has more than 3000 films, but many of which has missing information.\n",
    "\n",
    "The `preprocess.lua` **filters out all entries without any one of the following parameters**:\n",
    "\n",
    "- Film Name (`Title`)\n",
    "- Release Date (`Release Date`)\n",
    "- Production Budget (`Production Budget`)\n",
    "- Genre (`Main Genre`)\n",
    "- Worldwide Gross (`Worldwide Gross`)\n",
    "- Run Time (`Running Time min`)\n",
    "- IMDB Rating (`IMBD Rating`)\n",
    "\n",
    "After filtering, **1141 films** survived with **12 different genres**:\n",
    "\n",
    "```json\n",
    "[\"Action\",\"Black Comedy\",\"Comedy\",\"Adventure\",\"Drama\",\"Romantic Comedy\",\"Horror\",\"Thriller/Suspense\",\"Musical\",\"Documentary\",\"Western\",\"Concert/Performance\"]\n",
    "```\n",
    "\n",
    "The list of movies that survived is stored in `movies_processed.json`.\n",
    "\n",
    "The list of genres are stored in `genres.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a516e",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Before feeding the data into the FC neural network, we first need to find a way to encode the data into a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28f4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140 movies found with 12 genres.\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "with open(\"genres.json\") as f:\n",
    "    Genres = json.loads(f.read())\n",
    "with open(\"movies_processed.json\") as f:\n",
    "    Movies = json.loads(f.read())\n",
    "print(\"%d movies found with %d genres.\" % (len(Movies), len(Genres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3c304",
   "metadata": {},
   "source": [
    "### Input and Output\n",
    "\n",
    "We'll input all information of a movie into a FC network. But before doing so, proper encoding is necessary.\n",
    "\n",
    "#### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28f1581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN @ B (66)\n",
      "MAX @ B (66)\n",
      "MAX @ r (114)\n",
      "MIN @   (32)\n",
      "MAX @ w (119)\n",
      "MAX @ z (122)\n",
      "MAX @ È (200)\n",
      "MAX @ ‡ (8225)\n",
      "maxLen = 58\n",
      "maxAscii = 8225\n",
      "minAscii = 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "EncodedInput = []\n",
    "\n",
    "# Titles: stats\n",
    "maxLen = -1\n",
    "maxAscii = -1\n",
    "minAscii = np.Infinity\n",
    "for i in Movies:\n",
    "    t = str(i['Title'])\n",
    "    if len(t) > maxLen:\n",
    "        maxLen = len(t)\n",
    "    for char in t:\n",
    "        if ord(char) < minAscii:\n",
    "            minAscii = ord(char)\n",
    "            print(\"MIN @\", char, \"({0})\".format(ord(char)))\n",
    "        if ord(char) > maxAscii:\n",
    "            maxAscii = ord(char)\n",
    "            print(\"MAX @\", char, \"({0})\".format(ord(char)))\n",
    "print('maxLen = {0}\\nmaxAscii = {1}\\nminAscii = {2}'.format(maxLen, maxAscii, minAscii))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58473fc0",
   "metadata": {},
   "source": [
    "In the above snippet we found that the maximum length for a title is 58. Several strange symbols are found in addition to regular `(space)` to `z` in ASCII representations. To encode the title, any character with `ord(char) > 122 or ord(char) < 32` will be **clamped** into the range.\n",
    "\n",
    "The following code implements such encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7fb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broken Arrow [66, 114, 111, 107, 101, 110, 32, 65, 114, 114, 111, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Clamping function\n",
    "def clamp(val, valMin, valMax):\n",
    "    if val > valMax:\n",
    "        return valMax, True\n",
    "    elif val < valMin:\n",
    "        return valMin, True\n",
    "    else:\n",
    "        return val, False\n",
    "\n",
    "# ASCII encode\n",
    "TitleEncoded = list(list(clamp(ord(char), 32, 122)[0] for char in str(i[\"Title\"])) for i in Movies)\n",
    "# Extending to maxLen\n",
    "TitleEncoded = list(i + [0] * (maxLen - len(i)) for i in TitleEncoded)\n",
    "\n",
    "# Confirmation\n",
    "print(Movies[0][\"Title\"], TitleEncoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5200b5",
   "metadata": {},
   "source": [
    "The title in its ASCII form will be given as input to the neural network in **an array of 58 integers** ranging from 32 to 122.\n",
    "\n",
    "---\n",
    "\n",
    "Before moving on, define the following normalization function first. It maps an array of data into an array of float between `normMin` and `normMax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3420c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 114, 111, 107, 101, 110, 32, 65, 114, 114, 111, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Broken Arrow [0.5409836065573771, 0.9344262295081968, 0.9098360655737705, 0.8770491803278688, 0.8278688524590164, 0.9016393442622951, 0.26229508196721313, 0.5327868852459017, 0.9344262295081968, 0.9344262295081968, 0.9098360655737705, 0.9754098360655737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "def normalize(arr, normMin, normMax, inMin = -np.inf, inMax = np.inf):\n",
    "    if inMax == np.inf and inMin == -np.inf:\n",
    "        inMin, inMax = min(arr), max(arr)\n",
    "    return list(map(lambda val: (val - inMin) / (inMax - inMin) * (normMax - normMin) + normMin, arr))\n",
    "\n",
    "print(TitleEncoded[0])\n",
    "TitleEncoded = list(normalize(titleEncoded, 0, 1, 0, 122) for titleEncoded in TitleEncoded)\n",
    "print(Movies[0][\"Title\"], TitleEncoded[0])\n",
    "# Add to EncodedInput\n",
    "EncodedInput.append(TitleEncoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3fa1f",
   "metadata": {},
   "source": [
    "Then we'll move on to release dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "782f7a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release Date Timestamps ranges from 3296736.0 to 22074912.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Example: Jul 25 2008\n",
    "Timestamps = list(time.mktime(time.strptime(i[\"Release Date\"], \"%b %d %Y\")) / 100 for i in Movies)\n",
    "print(\"Release Date Timestamps ranges from\", min(Timestamps), \"to\", max(Timestamps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845b684",
   "metadata": {},
   "source": [
    "In the above snippet we found that the time starts with a big integer and has a rather big range. To normalize the time (foundamentally logical because time is consecutive), we set a linear normalization function to clamp all ranges into $[0,\\,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab596921",
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalizedTimestamps = normalize(Timestamps, 0, 1)\n",
    "# Add to EncodedInput\n",
    "EncodedInput.append(list([i] for i in NormalizedTimestamps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39208d19",
   "metadata": {},
   "source": [
    "The timestamps takes **1** float value when given as input to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90193ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21664838846239745 65000000\n"
     ]
    }
   ],
   "source": [
    "NormalizedProductionBudgets = normalize(list(i[\"Production Budget\"] for i in Movies), 0, 1)\n",
    "print(NormalizedProductionBudgets[0], Movies[0][\"Production Budget\"])\n",
    "EncodedInput.append(list([i] for i in NormalizedProductionBudgets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a426144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Action\n"
     ]
    }
   ],
   "source": [
    "def getGenreId(str):\n",
    "    return Genres.index(str)\n",
    "GenreId = list(map(getGenreId, list(i[\"Major Genre\"] for i in Movies)))\n",
    "print(GenreId[0], Movies[0][\"Major Genre\"])\n",
    "EncodedInput.append(list([i / len(Genres)] for i in GenreId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65fd0f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08049054258816266 148345997\n"
     ]
    }
   ],
   "source": [
    "NormalizedWorldwideGross = normalize(list(i[\"Worldwide Gross\"] for i in Movies), 0, 1)\n",
    "print(NormalizedWorldwideGross[0], Movies[0][\"Worldwide Gross\"])\n",
    "EncodedInput.append(list([i] for i in NormalizedWorldwideGross))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a2281a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3522727272727273 108\n"
     ]
    }
   ],
   "source": [
    "# EncodedInput.append(list([i[\"Running Time min\"]] for i in Movies))\n",
    "NormalizedRunTime = normalize(list(i[\"Running Time min\"] for i in Movies), 0, 1)\n",
    "print(NormalizedRunTime[0], Movies[0][\"Running Time min\"])\n",
    "EncodedInput.append(list([i] for i in NormalizedRunTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112895f5",
   "metadata": {},
   "source": [
    "Before finishing, let's check the final results in `EncodedInput`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bdc767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[[0.5409836065573771, 0.9344262295081968, 0.9098360655737705, 0.8770491803278688, 0.8278688524590164, 0.9016393442622951, 0.26229508196721313, 0.5327868852459017, 0.9344262295081968, 0.9344262295081968, 0.9098360655737705, 0.9754098360655737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26313610011962824], [0.21664838846239745], [0.0], [0.08049054258816266], [0.3522727272727273]] {'IMDB Votes': 33584, 'Production Budget': 65000000, 'IMDB Rating': 5.8, 'MPAA Rating': 'R', 'Rotten Tomatoes Rating': 55, 'Distributor': '20th Century Fox', 'Director': 'John Woo', 'Creative Type': 'Contemporary Fiction', 'US Gross': 70645997, 'Major Genre': 'Action', 'Worldwide Gross': 148345997, 'Source': 'Original Screenplay', 'Release Date': 'Feb 09 1996', 'Running Time min': 108, 'Title': 'Broken Arrow'}\n"
     ]
    }
   ],
   "source": [
    "print(len(EncodedInput))\n",
    "print(list(i[0] for i in EncodedInput), Movies[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2de54a",
   "metadata": {},
   "source": [
    "Now we can generate the encoded input for all movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "152e09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def concat(list1, list2):\n",
    "    list1.extend(list2)\n",
    "    return list1\n",
    "\n",
    "EncodedInput = list(reduce(concat, list(i[j] for i in EncodedInput)) for j in range(0, len(Movies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8d3c980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5409836065573771, 0.9344262295081968, 0.9098360655737705, 0.8770491803278688, 0.8278688524590164, 0.9016393442622951, 0.26229508196721313, 0.5327868852459017, 0.9344262295081968, 0.9344262295081968, 0.9098360655737705, 0.9754098360655737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26313610011962824, 0.21664838846239745, 0.0, 0.08049054258816266, 0.3522727272727273] {'IMDB Votes': 33584, 'Production Budget': 65000000, 'IMDB Rating': 5.8, 'MPAA Rating': 'R', 'Rotten Tomatoes Rating': 55, 'Distributor': '20th Century Fox', 'Director': 'John Woo', 'Creative Type': 'Contemporary Fiction', 'US Gross': 70645997, 'Major Genre': 'Action', 'Worldwide Gross': 148345997, 'Source': 'Original Screenplay', 'Release Date': 'Feb 09 1996', 'Running Time min': 108, 'Title': 'Broken Arrow'}\n"
     ]
    }
   ],
   "source": [
    "pointCount = len(EncodedInput)\n",
    "print(EncodedInput[0], Movies[0])\n",
    "inputDimension = len(EncodedInput[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ecbd6",
   "metadata": {},
   "source": [
    "### Groundtruths\n",
    "\n",
    "The groundtruth in this case is the IMDB rating of the 1140 movies. Simply compile the groundtruths based on the movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f29b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GroundTruth = normalize(list(round(i[\"IMDB Rating\"]) for i in Movies), 0, 10, 0, 10)\n",
    "assert(len(EncodedInput) == len(GroundTruth))\n",
    "\n",
    "def toOutTensor(result):\n",
    "    t = [0] * 10\n",
    "    t[int(result)] = 1\n",
    "    return t\n",
    "\n",
    "GroundTruth = list(map(toOutTensor, GroundTruth))\n",
    "Source = list((EncodedInput[i], GroundTruth[i]) for i in range(len(GroundTruth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97730e",
   "metadata": {},
   "source": [
    "## Constructing Neural Network\n",
    "\n",
    "Now it's time to make a neural network using basic PyTorch functions!\n",
    "\n",
    "Among the 1140 movies, ID 1-1000 is selected as *training set*, with batch size 25 (40 batches). ID 1001-1100 is *comfirmation set*, with batch size 100 (1 batch). ID 1101-1140 is the *test set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d291a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle sets\n",
    "def shuffleSource():\n",
    "    np.random.shuffle(Source)\n",
    "shuffleSource()\n",
    "\n",
    "# Configure batch size and count\n",
    "batchSize = 25\n",
    "batchCount = 40\n",
    "\n",
    "def getBatch(i):\n",
    "    if 25 * i > 1000:\n",
    "        assert()\n",
    "    else:\n",
    "        return Source[batchSize * i:batchSize * (i + 1)]\n",
    "\n",
    "def separateTensorsFromBatch(batch):\n",
    "    return torch.tensor(list(i[0] for i in batch)), torch.tensor(list(i[1] for i in batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b466b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "D_in, H, D_out = inputDimension, 100, 10\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 7.1e-5\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for epoch in range(300):\n",
    "#     print(\"------- EPOCH\", epoch, \"-------\")\n",
    "    shuffleSource()\n",
    "    for batchId in range(batchCount):\n",
    "        trainIn, trainOut = separateTensorsFromBatch(getBatch(batchId))\n",
    "        # Forward pass: compute predicted y\n",
    "        h = trainIn.mm(w1)\n",
    "        h_relu = h.clamp(min=0)\n",
    "        y_pred = h_relu.mm(w2)\n",
    "        y_pred_sm = F.softmax(y_pred, dim=1)\n",
    "        \n",
    "#       print(\"Output at case 0:\", y_pred[0])\n",
    "#       print(\"Expected output at case 0:\", trainOut[0])\n",
    "#       Compute and print loss\n",
    "        loss = (y_pred_sm - trainOut).pow(2).sum().item()\n",
    "        \n",
    "        # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "        grad_y_pred = 2.0 * (y_pred - trainOut)\n",
    "        grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "        grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "        grad_h = grad_h_relu.clone()\n",
    "        grad_h[h < 0] = 0\n",
    "        grad_w1 = trainIn.t().mm(grad_h)\n",
    "\n",
    "        # Update weights using gradient descent\n",
    "        w1 -= learning_rate * grad_w1\n",
    "        w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5e78bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: 7 (7) Confidence 12.07% -> 100.00%\n",
      "1001: 7 (7) Confidence 12.25% -> 100.00%\n",
      "1002: 7 (4) Confidence 11.85% -> 66.67%\n",
      "1003: 7 (7) Confidence 11.94% -> 75.00%\n",
      "1004: 8 (7) Confidence 12.63% -> 60.00%\n",
      "1005: 7 (6) Confidence 16.31% -> 50.00%\n",
      "1006: 7 (8) Confidence 11.74% -> 42.86%\n",
      "1007: 7 (6) Confidence 12.59% -> 37.50%\n",
      "1008: 7 (6) Confidence 12.19% -> 33.33%\n",
      "1009: 7 (8) Confidence 13.60% -> 30.00%\n",
      "1010: 8 (7) Confidence 13.40% -> 27.27%\n",
      "1011: 7 (5) Confidence 15.84% -> 25.00%\n",
      "1012: 7 (7) Confidence 12.77% -> 30.77%\n",
      "1013: 7 (4) Confidence 13.02% -> 28.57%\n",
      "1014: 7 (7) Confidence 11.86% -> 33.33%\n",
      "1015: 7 (8) Confidence 12.31% -> 31.25%\n",
      "1016: 7 (6) Confidence 11.66% -> 29.41%\n",
      "1017: 7 (7) Confidence 12.71% -> 33.33%\n",
      "1018: 7 (8) Confidence 13.44% -> 31.58%\n",
      "1019: 7 (6) Confidence 11.46% -> 30.00%\n",
      "1020: 7 (7) Confidence 10.91% -> 33.33%\n",
      "1021: 7 (7) Confidence 11.61% -> 36.36%\n",
      "1022: 7 (6) Confidence 12.55% -> 34.78%\n",
      "1023: 7 (6) Confidence 13.91% -> 33.33%\n",
      "1024: 7 (6) Confidence 12.59% -> 32.00%\n",
      "1025: 6 (7) Confidence 12.22% -> 30.77%\n",
      "1026: 7 (7) Confidence 11.24% -> 33.33%\n",
      "1027: 7 (8) Confidence 12.10% -> 32.14%\n",
      "1028: 7 (4) Confidence 12.94% -> 31.03%\n",
      "1029: 7 (7) Confidence 11.40% -> 33.33%\n",
      "1030: 7 (4) Confidence 12.01% -> 32.26%\n",
      "1031: 7 (8) Confidence 12.11% -> 31.25%\n",
      "1032: 7 (7) Confidence 14.08% -> 33.33%\n",
      "1033: 7 (6) Confidence 12.50% -> 32.35%\n",
      "1034: 7 (8) Confidence 13.91% -> 31.43%\n",
      "1035: 7 (7) Confidence 13.31% -> 33.33%\n",
      "1036: 7 (6) Confidence 13.51% -> 32.43%\n",
      "1037: 7 (5) Confidence 10.95% -> 31.58%\n",
      "1038: 7 (7) Confidence 12.93% -> 33.33%\n",
      "1039: 7 (6) Confidence 12.18% -> 32.50%\n",
      "1040: 7 (7) Confidence 15.21% -> 34.15%\n",
      "1041: 7 (7) Confidence 11.75% -> 35.71%\n",
      "1042: 7 (6) Confidence 13.31% -> 34.88%\n",
      "1043: 7 (7) Confidence 10.83% -> 36.36%\n",
      "1044: 7 (6) Confidence 12.50% -> 35.56%\n",
      "1045: 7 (6) Confidence 14.04% -> 34.78%\n",
      "1046: 7 (6) Confidence 12.57% -> 34.04%\n",
      "1047: 7 (7) Confidence 11.96% -> 35.42%\n",
      "1048: 7 (6) Confidence 12.56% -> 34.69%\n",
      "1049: 7 (6) Confidence 11.47% -> 34.00%\n",
      "1050: 7 (8) Confidence 12.63% -> 33.33%\n",
      "1051: 7 (7) Confidence 12.27% -> 34.62%\n",
      "1052: 7 (7) Confidence 12.48% -> 35.85%\n",
      "1053: 7 (6) Confidence 14.00% -> 35.19%\n",
      "1054: 7 (7) Confidence 12.25% -> 36.36%\n",
      "1055: 1 (7) Confidence 11.19% -> 35.71%\n",
      "1056: 8 (3) Confidence 13.22% -> 35.09%\n",
      "1057: 7 (6) Confidence 12.94% -> 34.48%\n",
      "1058: 7 (4) Confidence 15.98% -> 33.90%\n",
      "1059: 7 (8) Confidence 11.74% -> 33.33%\n",
      "1060: 7 (6) Confidence 12.34% -> 32.79%\n",
      "1061: 7 (8) Confidence 15.35% -> 32.26%\n",
      "1062: 7 (6) Confidence 11.79% -> 31.75%\n",
      "1063: 7 (7) Confidence 12.39% -> 32.81%\n",
      "1064: 6 (5) Confidence 13.24% -> 32.31%\n",
      "1065: 7 (8) Confidence 12.66% -> 31.82%\n",
      "1066: 8 (8) Confidence 16.61% -> 32.84%\n",
      "1067: 7 (7) Confidence 13.28% -> 33.82%\n",
      "1068: 7 (6) Confidence 11.99% -> 33.33%\n",
      "1069: 7 (7) Confidence 11.94% -> 34.29%\n",
      "1070: 7 (7) Confidence 12.21% -> 35.21%\n",
      "1071: 7 (6) Confidence 12.32% -> 34.72%\n",
      "1072: 7 (6) Confidence 15.63% -> 34.25%\n",
      "1073: 7 (8) Confidence 11.85% -> 33.78%\n",
      "1074: 7 (7) Confidence 12.35% -> 34.67%\n",
      "1075: 7 (6) Confidence 14.61% -> 34.21%\n",
      "1076: 7 (6) Confidence 12.84% -> 33.77%\n",
      "1077: 7 (7) Confidence 14.03% -> 34.62%\n",
      "1078: 5 (4) Confidence 12.69% -> 34.18%\n",
      "1079: 7 (8) Confidence 10.86% -> 33.75%\n",
      "1080: 7 (6) Confidence 11.45% -> 33.33%\n",
      "1081: 7 (7) Confidence 11.21% -> 34.15%\n",
      "1082: 7 (7) Confidence 12.49% -> 34.94%\n",
      "1083: 7 (8) Confidence 14.53% -> 34.52%\n",
      "1084: 8 (7) Confidence 12.26% -> 34.12%\n",
      "1085: 7 (6) Confidence 14.11% -> 33.72%\n",
      "1086: 6 (7) Confidence 13.35% -> 33.33%\n",
      "1087: 7 (8) Confidence 13.92% -> 32.95%\n",
      "1088: 7 (8) Confidence 11.84% -> 32.58%\n",
      "1089: 7 (4) Confidence 12.95% -> 32.22%\n",
      "1090: 7 (5) Confidence 12.68% -> 31.87%\n",
      "1091: 8 (6) Confidence 11.43% -> 31.52%\n",
      "1092: 7 (5) Confidence 12.52% -> 31.18%\n",
      "1093: 8 (8) Confidence 18.64% -> 31.91%\n",
      "1094: 7 (8) Confidence 11.91% -> 31.58%\n",
      "1095: 7 (6) Confidence 12.04% -> 31.25%\n",
      "1096: 7 (7) Confidence 11.63% -> 31.96%\n",
      "1097: 7 (5) Confidence 11.79% -> 31.63%\n",
      "1098: 7 (7) Confidence 12.37% -> 32.32%\n",
      "1099: 7 (8) Confidence 11.96% -> 32.00%\n",
      "1100: 7 (7) Confidence 11.00% -> 32.67%\n",
      "1101: 7 (6) Confidence 12.27% -> 32.35%\n",
      "1102: 7 (5) Confidence 12.59% -> 32.04%\n",
      "1103: 7 (7) Confidence 14.51% -> 32.69%\n",
      "1104: 6 (6) Confidence 11.87% -> 33.33%\n",
      "1105: 7 (6) Confidence 13.20% -> 33.02%\n",
      "1106: 7 (8) Confidence 12.24% -> 32.71%\n",
      "1107: 7 (6) Confidence 12.06% -> 32.41%\n",
      "1108: 7 (6) Confidence 12.42% -> 32.11%\n",
      "1109: 7 (7) Confidence 11.82% -> 32.73%\n",
      "1110: 7 (7) Confidence 12.64% -> 33.33%\n",
      "1111: 7 (5) Confidence 14.71% -> 33.04%\n",
      "1112: 10 (6) Confidence 12.42% -> 32.74%\n",
      "1113: 7 (7) Confidence 13.41% -> 33.33%\n",
      "1114: 7 (8) Confidence 12.12% -> 33.04%\n",
      "1115: 7 (5) Confidence 11.08% -> 32.76%\n",
      "1116: 7 (6) Confidence 15.18% -> 32.48%\n",
      "1117: 7 (8) Confidence 11.33% -> 32.20%\n",
      "1118: 7 (5) Confidence 16.15% -> 31.93%\n",
      "1119: 7 (6) Confidence 14.47% -> 31.67%\n",
      "1120: 7 (6) Confidence 12.02% -> 31.40%\n",
      "1121: 7 (8) Confidence 11.71% -> 31.15%\n",
      "1122: 7 (6) Confidence 13.53% -> 30.89%\n",
      "1123: 7 (5) Confidence 14.41% -> 30.65%\n",
      "1124: 7 (7) Confidence 12.68% -> 31.20%\n",
      "1125: 7 (8) Confidence 11.61% -> 30.95%\n",
      "1126: 7 (7) Confidence 13.44% -> 31.50%\n",
      "1127: 2 (8) Confidence 14.23% -> 31.25%\n",
      "1128: 7 (6) Confidence 11.91% -> 31.01%\n",
      "1129: 7 (6) Confidence 13.45% -> 30.77%\n",
      "1130: 7 (7) Confidence 11.60% -> 31.30%\n",
      "1131: 7 (5) Confidence 14.39% -> 31.06%\n",
      "1132: 7 (8) Confidence 12.54% -> 30.83%\n",
      "1133: 7 (5) Confidence 11.44% -> 30.60%\n",
      "1134: 7 (6) Confidence 14.43% -> 30.37%\n",
      "1135: 7 (8) Confidence 11.61% -> 30.15%\n",
      "1136: 3 (7) Confidence 16.24% -> 29.93%\n",
      "1137: 7 (8) Confidence 11.84% -> 29.71%\n",
      "1138: 7 (7) Confidence 11.95% -> 30.22%\n",
      "1139: 7 (6) Confidence 12.43% -> 30.00%\n"
     ]
    }
   ],
   "source": [
    "def pred(inputArr):\n",
    "    inputTensor = torch.tensor([inputArr])\n",
    "    # Forward pass: compute predicted y\n",
    "    h = inputTensor.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    y_pred_sm = F.softmax(y_pred, dim=1)\n",
    "    arr = list(float(i) for i in y_pred_sm[0])\n",
    "    return arr.index(max(arr)) + 1, max(arr)\n",
    "\n",
    "correct = 0\n",
    "case = 0\n",
    "\n",
    "for i in range(1000, 1140):\n",
    "    case += 1\n",
    "    p, q = pred(Source[i][0])\n",
    "    a = Source[i][1].index(1)\n",
    "    correct += (p==a)\n",
    "    if case != 0:\n",
    "        print(\"%d: %d (%d) Confidence %.2f%% -> %.2f%%\" % (i, p, a, q*100, correct/case * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e192bc6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The model finally achieves an accuracy of around 30%. This is almost the best result!\n",
    "\n",
    "(Further conclusion will be updated in the future)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
