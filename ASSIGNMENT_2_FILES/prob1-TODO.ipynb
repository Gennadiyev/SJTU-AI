{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 问题1：请使用numpy来实现梯度下降并做一个单变量的线性回归。使用prob1.dat数据来验证你的算法。\n",
    "#### 下面，我们将使用y = wX + b来表示该线性回归。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import sparse\n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "'''\n",
    "This problem is adapted from the course materials of IST 597 at Penn State University.\n",
    "Thanks to the contribution of Alexander G. Ororbia II.\n",
    "'''\n",
    "\n",
    "# NOTE: 更新的步长、收敛标准和更新轮数\n",
    "# meta-parameters for program\n",
    "alpha = 0.01 # step size coefficient\n",
    "eps = 1e-8 # controls convergence criterion\n",
    "n_epoch = 1000 # number of epochs (full passes through the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress(X, theta):\n",
    "    # 给定X和theta,输出y的预测值\n",
    "    m = np.shape(X)[0]\n",
    "    print(\"M=\",m)\n",
    "    input()\n",
    "    b = theta[0]\n",
    "    w = theta[1]\n",
    "    y_hat = np.repeat(b[None,:], m, axis = 0) + np.dot(X, w.transpose())\n",
    "    return y_hat\n",
    "\n",
    "def computeCost(X,y,theta,reg):\n",
    "    N = X.shape[0]\n",
    "    y_mat = oneHotIt(y)\n",
    "    probs = predict(X,theta)[1]\n",
    "    loss = -np.sum(y_mat*np.log(probs))/N + (reg/2)*np.sum(theta[0]**2)   \n",
    "    return loss\n",
    "\n",
    "def computeCost(X, y, theta):\n",
    "    # 计算当前的Rooted Mean Squared Error\n",
    "    m = np.shape(y)[0]\n",
    "    y_hat = regress(X, theta)\n",
    "    loss = 1/(2*m)*np.sum(np.square(y_hat - y))\n",
    "    return loss\n",
    "\n",
    "def oneHotIt(y):\n",
    "    N = y.shape[0]\n",
    "    M = []\n",
    "    for i in y:\n",
    "        M.append([round(i[0])])\n",
    "    M = np.mat(M)\n",
    "    print(M)\n",
    "    OHX = sparse.csr_matrix((np.ones(N), (M, np.array(range(N)))))\n",
    "    OHX = np.array(OHX.todense()).T\n",
    "    return OHX\n",
    "\n",
    "def predict(X,theta):\n",
    "    W = theta[0]\n",
    "    b = theta[1]\n",
    "    scores = np.dot(X,W)+b\n",
    "    probs = np.apply_along_axis(softmax, 1, scores)\n",
    "    return (scores, probs)\n",
    "\n",
    "def computeGrad(X, y, theta): \n",
    "    dL_db = 0\n",
    "    dL_dw = 0\n",
    "    N = X.shape[0]\n",
    "    y_mat = oneHotIt(y)\n",
    "    probs = predict(X,theta)[1]\n",
    "    ddy = (probs-y_mat)\n",
    "    dL_dw = np.dot(X.T, ddy)/N + regress(X, theta)*theta[0] \n",
    "    dL_db = np.sum(ddy, axis=0)/N\n",
    "    nabla = (dL_db, dL_dw)\n",
    "    return nabla    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始\n",
    "\n",
    "path = '/storage/emulated/0/Kunologist/Jupyter/LectureNotes/ASSIGNMENT_2_FILES/prob1.dat'  \n",
    "data = pd.read_csv(path, header=None, names=['X', 'Y']) \n",
    "\n",
    "data.describe()\n",
    "\n",
    "# 转换数据\n",
    "cols = data.shape[1]  \n",
    "X = data.iloc[:,0:cols-1]  \n",
    "y = data.iloc[:,cols-1:cols] \n",
    "\n",
    "X = np.array(X.values)  \n",
    "y = np.array(y.values)\n",
    "\n",
    "# 初始化线性回归的参数\n",
    "w = np.zeros((1,X.shape[1]))\n",
    "b = np.array([0])\n",
    "theta = (b, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M= 97\n"
     ]
    }
   ],
   "source": [
    "# 第一次计算loss\n",
    "L = computeCost(X, y, theta)\n",
    "print(\"-1 L = {0}\".format(L))\n",
    "L_best = L\n",
    "i = 0\n",
    "cost = []\n",
    "while(i < n_epoch):\n",
    "    # TODO - 计算梯度，更新theta，计算loss，打印loss\n",
    "    dL_dw, dL_db = computeGrad(X, y, theta)\n",
    "    b = theta[1]\n",
    "    w = theta[0]\n",
    "    new_b = b - (alpha * dL_db)\n",
    "    new_w = w - (alpha * dL_dw)\n",
    "    theta = (new_w, new_b)\n",
    "    L = computeCost(X, y, theta)\n",
    "    print(\" {0} L = {1}\".format(i,L))\n",
    "    i += 1\n",
    "    \n",
    "    cost.append(L)\n",
    "    L_best = min(L_best, L)\n",
    "    \n",
    "# print parameter values found after the search\n",
    "print(\"w = \",w)\n",
    "print(\"b = \",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出数据点\n",
    "plt.scatter(X[:,0], y, s=30, label=\"Samples\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(os.path.join(\"prob1.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出数据点和拟合的曲线\n",
    "plt.plot(X_test, regress(X_test, theta), color = 'r', label=\"Model\")\n",
    "plt.scatter(X[:,0], y, s=30, label=\"Samples\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((np.amin(X_test) - kludge, np.amax(X_test) + kludge))\n",
    "plt.ylim((np.amin(y) - kludge, np.amax(y) + kludge))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(str(alpha) + \"_fit.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画出loss收敛过程的曲线\n",
    "plt.plot([i+1 for i in range(len(cost))], cost, label = \"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(str(alpha) + \"_loss.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
